# scripts/run_test.py
import os
import sys
import time
import argparse
import asyncio
import queue
import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ëª¨ë“ˆ importë¥¼ ìœ„í•´)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "src")))


# ì‹¤ì œ ëª¨ë¸ í´ë˜ìŠ¤ (ê²½ë¡œëŠ” ìë„¤ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ)
from src.inference import Qwen3OmniFullDuplexEngine, EngineConfig
from transformers import Qwen3OmniMoeForConditionalGeneration
from transformers import Qwen3OmniMoeProcessor

def load_audio_file(file_path, target_sr=24000):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¦¬ìƒ˜í”Œë§í•¨"""
    print(f"ğŸ“‚ Loading audio file: {file_path}")
    # librosaëŠ” float32 [-1, 1]ë¡œ ë¡œë“œí•¨
    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
    return audio, sr

def main():
    parser = argparse.ArgumentParser(description="Test Full-Duplex Engine with Audio File")
    parser.add_argument("--model-path", type=str, default="Qwen/Qwen3-Omni-30B-A3B-Instruct", help="Path to model")
    parser.add_argument("--input-file", type=str, required=True, help="Input audio file (e.g. 3min_noisy.wav)")
    parser.add_argument("--output-file", type=str, default="output_response.wav", help="Output audio file")
    parser.add_argument("--device", type=str, default="cuda", help="Device to run")
    args = parser.parse_args()

    # 1. ëª¨ë¸ ë¡œë“œ (run.pyì™€ ë™ì¼í•œ ë°©ì‹)
    print("ğŸ”¥ Loading Model...")
    
    # (ì„ íƒ) Multi-GPU ë¡œë“œ ë¡œì§ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€
    # device_map = load_distributed_map() 
    
    model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
        model_path,
        device_map=device_map, # ë˜ëŠ” "auto"
        dtype='auto',          # torch.float16 ë˜ëŠ” bfloat16 ìë™ ì„ íƒ
        attn_implementation='flash_attention_2', 
        trust_remote_code=True
    )
    
    # 3. í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = Qwen3OmniMoeProcessor.from_pretrained(model_path, trust_remote_code=True)
    
 
    # 2. ì—”ì§„ ì´ˆê¸°í™”
    config = EngineConfig(audio_input_tokens=4, text_output_tokens=2, audio_output_tokens=4)
    engine = Qwen3OmniFullDuplexEngine(model, processor, config)
    
    # 3. ì˜¤ë””ì˜¤ ì¤€ë¹„ (Chunking)
    full_audio, sr = load_audio_file(args.input_file, target_sr=24000)
    
    # 4í† í° ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (ì˜ˆ: 0.32ì´ˆ)
    # Qwen3-Omniì˜ í”„ë ˆì„ ì†ë„ì— ë§ì¶°ì•¼ í•¨. (ê°€ì •: 12.5Hz -> 1í”„ë ˆì„ë‹¹ 0.08ì´ˆ)
    # 4í† í° = 0.32ì´ˆ = 24000 * 0.32 = 7680 ìƒ˜í”Œ
    chunk_size = int(sr * 0.32) 
    
    chunks = [full_audio[i:i + chunk_size] for i in range(0, len(full_audio), chunk_size)]
    print(f"ğŸ“¦ Audio split into {len(chunks)} chunks (Chunk size: {chunk_size} samples)")

    # 4. í…ŒìŠ¤íŠ¸ ì‹œì‘
    engine.start()
    
    collected_output_audio = []
    start_time = time.time()
    
    try:
        # -- [Sender Loop] ì˜¤ë””ì˜¤ë¥¼ ì‹¤ì‹œê°„ì²˜ëŸ¼ ì¡°ê¸ˆì”© ë°€ì–´ë„£ìŒ --
        for i, chunk in enumerate(chunks):
            # ë§ˆì§€ë§‰ ì§œíˆ¬ë¦¬ íŒ¨ë”© (í•„ìš”ì‹œ)
            if len(chunk) < chunk_size:
                chunk = np.pad(chunk, (0, chunk_size - len(chunk)))
            
            # Tensor ë³€í™˜ [1, T, D] ë“± ëª¨ë¸ ì¸í’‹ í˜•íƒœì— ë§ê²Œ (Audio Encoderê°€ ìˆë‹¤ê³  ê°€ì •)
            # ì—¬ê¸°ì„œëŠ” Raw Audioë¥¼ Encoderì— ë„£ê¸° ì „ ë‹¨ê³„ë¼ê³  ê°€ì •í•˜ê³  Tensorë¡œë§Œ ë³€í™˜
            # ì‹¤ì œë¡œëŠ” model.audio_encoder(chunk)ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜, ì—”ì§„ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•¨.
            # ì—”ì§„ ì½”ë“œì˜ push_audioëŠ” "Audio Features"ë¥¼ ë°›ìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œ ì¸ì½”ë”©ì„ í•´ì¤˜ì•¼ í•¨.
            
            audio_tensor = torch.from_numpy(chunk).float().to(args.device)
            
            # [ì¤‘ìš”] Audio Encoder í†µê³¼ (Engine ì™¸ë¶€ì—ì„œ í• ì§€ ë‚´ë¶€ì—ì„œ í• ì§€ ê²°ì • í•„ìš”)
            # Moshi í…ŒìŠ¤íŠ¸ ì½”ë“œì²˜ëŸ¼ ì—¬ê¸°ì„œ ì¸ì½”ë”©í•´ì„œ 'Feature'ë¥¼ ë„˜ê¸°ëŠ” ê²Œ ì •ì„
            with torch.no_grad():
                # Qwen3 Audio Encoder í˜¸ì¶œ (ê°€ì •: input_values=[1, len])
                # ì‹¤ì œ ëª¨ë¸ì˜ processorë‚˜ encoder ë©”ì„œë“œ í™•ì¸ í•„ìš”
                # ì˜ˆì‹œ: audio_features = model.audio_tower(audio_tensor.unsqueeze(0))
                
                # ì„ì‹œ: ë‹¨ìˆœíˆ ì°¨ì›ë§Œ ë§ì¶°ì„œ ë³´ëƒ„ (ì‹¤ì œ í™˜ê²½ì—ì„  Encoder í˜¸ì¶œ í•„ìˆ˜!)
                # audio_features = audio_tensor.view(1, 1, -1) 
                
                # [ìˆ˜ì •] ìë„¤ ëª¨ë¸ì˜ Audio Encoder ì‚¬ìš©
                # Qwen3-Omni Audio Encoderê°€ mel-spectrogramì„ ë°›ëŠ”ì§€, raw waveë¥¼ ë°›ëŠ”ì§€ í™•ì¸
                # ì—¬ê¸°ì„œëŠ” 'audio_tower'ê°€ featureë¥¼ ë½‘ì•„ì¤€ë‹¤ê³  ê°€ì •
                audio_features = model.audio_tower(audio_tensor.unsqueeze(0)) # [1, 4, Dim]
            
            # ì—”ì§„ì— íˆ¬ì…
            engine.push_audio(audio_features)
            
            # ì‹¤ì‹œê°„ì„± ì‹œë®¬ë ˆì´ì…˜ (0.32ì´ˆ ëŒ€ê¸°)
            # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ì²˜ëŸ¼ ì²œì²œíˆ ë„£ìŒ (í…ŒìŠ¤íŠ¸ ì†ë„ ë†’ì´ë ¤ë©´ ì£¼ì„ ì²˜ë¦¬)
            # time.sleep(0.32) 
            
            # -- [Receiver Loop] ìƒì„±ëœ ì˜¤ë””ì˜¤ ìˆ˜ê±° --
            # ë…¼ë¸”ë¡œí‚¹ìœ¼ë¡œ í™•ì¸
            while True:
                out_bytes = engine.get_audio_output()
                if out_bytes is None:
                    break
                
                # Bytes -> Numpy ë³€í™˜
                out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
                collected_output_audio.append(out_np)
                print(f"ğŸ”Š Received output chunk ({len(out_np)} samples)")

        # ëª¨ë“  ì…ë ¥ ì „ì†¡ í›„ ì ì‹œ ëŒ€ê¸° (ì”ì—¬ ì¶œë ¥ ìˆ˜ê±°)
        print("â³ Waiting for remaining outputs...")
        time.sleep(2.0) 
        
        # ë‚¨ì€ê±° ì‹¹ ê¸ì–´ëª¨ìœ¼ê¸°
        while True:
            out_bytes = engine.get_audio_output()
            if out_bytes is None: break
            out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
            collected_output_audio.append(out_np)

    except KeyboardInterrupt:
        print("ğŸ›‘ Test interrupted")
    finally:
        engine.stop()
    
    # 5. ê²°ê³¼ ì €ì¥
    if collected_output_audio:
        final_audio = np.concatenate(collected_output_audio)
        print(f"ğŸ’¾ Saving {len(final_audio)} samples to {args.output_file}")
        sf.write(args.output_file, final_audio, 24000)
    else:
        print("âš ï¸ No audio generated!")

    print(f"âœ… Test Finished. Total time: {time.time() - start_time:.2f}s")

if __name__ == "__main__":
    main()