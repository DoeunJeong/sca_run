import os
import sys
import time
import argparse
import threading  # ë©€í‹°ì“°ë ˆë“œìš©
import torch
import numpy as np
import librosa
import soundfile as sf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# íŒ¨í‚¤ì§€ ê²½ë¡œ (sca_core)
from src.inference import Qwen3OmniFullDuplexEngine, EngineConfig
from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor

def load_audio_file(file_path, target_sr=16000):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¦¬ìƒ˜í”Œë§í•¨ (Whisper ì…ë ¥ìš© 16kHz)"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Audio file not found: {file_path}")
    print(f"ğŸ“‚ Loading audio file: {file_path}")
    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
    return audio, sr

# -----------------------------------------------------------------------------
# [Receiver Thread] ì—”ì§„ì—ì„œ ë‚˜ì˜¤ëŠ” ì˜¤ë””ì˜¤ë¥¼ ë³„ë„ ì“°ë ˆë“œë¡œ ê³„ì† ìˆ˜ê±°
# -----------------------------------------------------------------------------
def audio_receiver_loop(engine, collected_list, stop_event):
    print("ğŸ§ [Receiver] Listening for output...")
    while not stop_event.is_set():
        # Non-blockingìœ¼ë¡œ í™•ì¸
        out_bytes = engine.get_audio_output()
        if out_bytes:
            # Bytes -> Float32 ë³€í™˜
            out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
            collected_list.append(out_np)
            print(".", end="", flush=True) # ì§„í–‰ ìƒí™© í‘œì‹œ
        else:
            time.sleep(0.001) # CPU ì–‘ë³´

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, default="Qwen/Qwen3-Omni-30B-A3B-Instruct")
    parser.add_argument("--input-file", type=str, required=True)
    parser.add_argument("--output-file", type=str, default="output_response.wav")
    parser.add_argument("--device", type=str, default="cuda")
    args = parser.parse_args()

    # 1. ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”¥ Loading Model from {args.model_path}...")
    model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
        args.model_path,
        device_map="auto",
        dtype='auto',
        attn_implementation='flash_attention_2',
        trust_remote_code=True
    )
    processor = Qwen3OmniMoeProcessor.from_pretrained(args.model_path, trust_remote_code=True)
    
    # 2. ì—”ì§„ ì´ˆê¸°í™”
    config = EngineConfig(audio_input_tokens=4, text_output_tokens=2, audio_output_tokens=4)
    engine = Qwen3OmniFullDuplexEngine(model, processor.tokenizer, config)
    
    # 3. ì˜¤ë””ì˜¤ ì¤€ë¹„ (16kHz)
    full_audio, sr = load_audio_file(args.input_file, target_sr=16000)
    chunk_size = int(sr * 0.32) # 0.32ì´ˆ ë‹¨ìœ„
    chunks = [full_audio[i:i + chunk_size] for i in range(0, len(full_audio), chunk_size)]
    print(f"ğŸ“¦ Input Audio Split: {len(chunks)} chunks (0.32s each)")

    # 4. ì—”ì§„ ì‹œì‘
    engine.start()
    
    # 5. [Receiver Thread] ì‹œì‘ (ë¹„ë™ê¸° ìˆ˜ì‹ )
    collected_output_audio = []
    stop_receiver = threading.Event()
    receiver_thread = threading.Thread(
        target=audio_receiver_loop, 
        args=(engine, collected_output_audio, stop_receiver),
        daemon=True
    )
    receiver_thread.start()
    
    start_time = time.time()
    
    try:
        # 6. [Sender Loop] ë©”ì¸ ì“°ë ˆë“œëŠ” ì˜¤ë””ì˜¤ ë°€ì–´ë„£ê¸°ë§Œ ìˆ˜í–‰
        print("ğŸ™ï¸ [Sender] Streaming audio chunks...")
        for i, chunk in enumerate(chunks):
            if len(chunk) < chunk_size:
                chunk = np.pad(chunk, (0, chunk_size - len(chunk)))
            
            with torch.no_grad():
                # â˜… [ìˆ˜ì •] Audio Tower ì§ì ‘ í˜¸ì¶œ X -> Feature Extractor ì‚¬ìš©
                # Raw Audio(16k) -> Mel Spectrogram ë³€í™˜
                features = processor.feature_extractor(
                    [chunk], 
                    return_tensors="pt", 
                    sampling_rate=16000
                )
                # [Batch, Mel, Time] -> GPU ì´ë™
                input_features = features.input_features.to(args.device).to(model.dtype)
            
            # ì—”ì§„ì— Feature íˆ¬ì… (Non-blocking)
            engine.push_audio(input_features)
            
            # (ì˜µì…˜) ì‹¤ì‹œê°„ì„± ì‹œë®¬ë ˆì´ì…˜: 0.32ì´ˆ ëŒ€ê¸°
            # time.sleep(0.32) 

        print("\nâœ… [Sender] All chunks sent. Waiting for trailing response...")
        
        # 7. ì”ì—¬ ì‘ë‹µ ëŒ€ê¸° (3ì´ˆ)
        time.sleep(3.0)

    except KeyboardInterrupt:
        print("\nğŸ›‘ Test interrupted")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ì¢…ë£Œ ì²˜ë¦¬
        stop_receiver.set()
        receiver_thread.join()
        engine.stop()
    
    # 8. ê²°ê³¼ ì €ì¥
    if collected_output_audio:
        final_audio = np.concatenate(collected_output_audio)
        OUTPUT_SR = 24000 
        print(f"ğŸ’¾ Saving {len(final_audio)} samples ({len(final_audio)/OUTPUT_SR:.1f}s) to {args.output_file}")
        sf.write(args.output_file, final_audio, OUTPUT_SR)
    else:
        print("âš ï¸ No output received!")

    print(f"â±ï¸ Total Time: {time.time() - start_time:.2f}s")

if __name__ == "__main__":
    main()