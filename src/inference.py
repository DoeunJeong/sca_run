import torch
import numpy as np
import threading
import queue
import time
from dataclasses import dataclass
from typing import Optional, List, Any

# =============================================================================
# 1. ì„¤ì • ë° ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================
@dataclass
class EngineConfig:
    audio_input_tokens: int = 4   
    text_output_tokens: int = 2   
    audio_output_tokens: int = 4  
    silence_token_id: int = 151646 
    
    system_prompt_text: str = (
        "<|im_start|>system\n"
        "You are a funny comedian performing a stand-up comedy show using Qwen3-Omni.\n"
        "<|im_end|>\n"
    )

# =============================================================================
# 2. ë¡œì§ í´ë˜ìŠ¤
# =============================================================================
class Qwen3DuplexLogic:
    def __init__(self, model):
        self.model = model
        self.device = model.device
        
        self.thinker_device = model.thinker.device
        self.talker_device = model.talker.device
        self.code2wav_device = model.code2wav.device
        
        self.talker_config = model.config.talker_config
        # â˜… [ìˆ˜ì •] ëª¨ë¸ ì„¤ì •ì—ì„œ Codec Layer ê°œìˆ˜ í™•ì¸ (ê¸°ë³¸ê°’ 16)
        self.num_quantizers = getattr(self.talker_config, "num_quantizers", 16)
        
        # Audio Tower Dtype í™•ì¸
        try:
            self.audio_dtype = model.thinker.audio_tower.conv2d1.weight.dtype
        except:
            self.audio_dtype = model.dtype

    @torch.no_grad()
    def thinker_step(
        self,
        input_ids: Optional[torch.Tensor],
        input_features: Optional[torch.Tensor], # â˜… ìˆ˜ì •: Audio Features ë°›ìŒ
        feature_attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[List],
        step_idx: int
    ):
        # [Multi-GPU Safety]
        if input_ids is not None and input_ids.device != self.thinker_device:
            input_ids = input_ids.to(self.thinker_device)
        if input_features is not None:
            if input_features.device != self.thinker_device:
                input_features = input_features.to(self.thinker_device)
            # Dtype ë§ì¶¤
            input_features = input_features.to(dtype=self.audio_dtype)
        if feature_attention_mask is not None and feature_attention_mask.device != self.thinker_device:
            feature_attention_mask = feature_attention_mask.to(self.thinker_device)

        # RoPE Position IDs ìƒì„±
        # Audio ì…ë ¥ ì‹œ: feature ê¸¸ì´ë§Œí¼ / Text ì…ë ¥ ì‹œ: text ê¸¸ì´ë§Œí¼
        if input_ids is None and input_features is not None:
            # ë”ë¯¸ í† í° (ì˜ˆ: íŒ¨ë”© í† í°ì´ë‚˜ <|audio|> í† í° ë“±)
            # ì—¬ê¸°ì„  ë‹¨ìˆœíˆ ê¸¸ì´ 1ì§œë¦¬ í…ì„œë¥¼ ë§Œë“¤ê³  ë¬´ì‹œë˜ê¸¸ ê¸°ëŒ€í•˜ê±°ë‚˜,
            # ëª¨ë¸ì´ input_featuresê°€ ìˆìœ¼ë©´ input_idsë¥¼ ë¬´ì‹œí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆëŠ”ì§€ í™•ì¸ í•„ìš”.
            # ê°€ì¥ ì•ˆì „í•œ ê±´: input_idsì— <|audio|> í† í° í•˜ë‚˜ ë„£ì–´ì£¼ëŠ” ê²ƒ.
            
            # 151646 ë“± íŠ¹ìˆ˜ í† í° ì‚¬ìš©? ê·¸ëƒ¥ 0ë²ˆ í† í° ì‚¬ìš©
            input_ids = torch.tensor([[0]], device=self.thinker_device)

        position_ids = torch.tensor([[step_idx]], device=self.thinker_device)
        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        # Thinker Forward
        # ì„±ê³µí–ˆë˜ ì½”ë“œì²˜ëŸ¼ input_featuresë¥¼ ì§ì ‘ ë„˜ê¹€
        outputs = self.model.thinker(
            input_ids=input_ids,
            input_features=input_features,
            feature_attention_mask=feature_attention_mask,
            past_key_values=past_key_values,
            position_ids=position_ids,
            use_cache=True,
            output_hidden_states=True
        )
        
        return outputs

    @torch.no_grad()
    def talker_step(
        self,
        thinker_hidden: torch.Tensor,
        past_key_values: Optional[List],
        step_idx: int,
        input_ids: Optional[torch.Tensor] = None
    ):
        if thinker_hidden.device != self.talker_device:
            thinker_hidden = thinker_hidden.to(self.talker_device)
        
        if input_ids is None:
             input_ids = torch.tensor([[self.model.config.talker_config.codec_bos_id]], device=self.talker_device)
        else:
             input_ids = input_ids.to(self.talker_device)

        conditioned_hidden = self.model.talker.text_projection(thinker_hidden)
        audio_embed = self.model.talker.model.get_input_embeddings()(input_ids)
        talker_inputs_embeds = audio_embed + conditioned_hidden
        
        position_ids = torch.tensor([[step_idx]], device=self.talker_device)
        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        talker_out = self.model.talker.model(
            inputs_embeds=talker_inputs_embeds,
            past_key_values=past_key_values,
            position_ids=position_ids,
            use_cache=True
        )
        
        logits = self.model.talker.codec_head(talker_out.last_hidden_state[:, -1, :])
        layer0_code = logits.argmax(dim=-1, keepdim=True)
        
        last_id_hidden = self.model.talker.get_input_embeddings()(layer0_code)
        past_hidden = talker_out.last_hidden_state[:, -1:]
        predictor_input = torch.cat((past_hidden, last_id_hidden), dim=1)
        
        # â˜… [ìˆ˜ì •] ì „ì²´ 16ê°œ ì¤‘ 1ê°œ(Layer0)ëŠ” ì´ë¯¸ ë‚˜ì™”ìœ¼ë¯€ë¡œ 15ê°œë¥¼ ë” ìƒì„±í•´ì•¼ í•¨
        needed_tokens = self.num_quantizers - 1
        
        predictor_out = self.model.talker.code_predictor.generate(
            inputs_embeds=predictor_input,
            max_new_tokens=needed_tokens, # 7 -> needed_tokens (15)ë¡œ ë³€ê²½
            do_sample=False
        )
        
        full_audio_codes = torch.cat([layer0_code, predictor_out], dim=1)
        return full_audio_codes, talker_out.past_key_values

    @torch.no_grad()
    def decode_audio(self, audio_codes: torch.Tensor) -> bytes:
        if audio_codes.device != self.code2wav_device:
            audio_codes = audio_codes.to(self.code2wav_device)
        if audio_codes.dim() == 2:
            audio_codes = audio_codes.unsqueeze(-1)
            
        wav_tensor = self.model.code2wav(audio_codes)
        wav_np = wav_tensor.cpu().float().numpy()
        wav_int16 = (wav_np * 32767).astype(np.int16)
        return wav_int16.tobytes()

# =============================================================================
# 3. ì—”ì§„ í´ë˜ìŠ¤
# =============================================================================
class Qwen3OmniFullDuplexEngine:
    def __init__(self, model, tokenizer, config: EngineConfig):
        self.model = model
        self.tokenizer = tokenizer
        self.cfg = config
        self.logic = Qwen3DuplexLogic(model)
        
        # Queues
        self.input_queue = queue.Queue()   
        self.hidden_queue = queue.Queue()  
        self.output_queue = queue.Queue()  
        
        # States
        self.thinker_kv_cache = None
        self.talker_kv_cache = None
        self.text_history_ids = None 
        self.last_talker_token = None
        
        self.thinker_step_count = 0
        self.talker_step_count = 0
        
        self.is_running = False
        self.t_thinker = None
        self.t_talker = None

        self._initialize_context()

    def _initialize_context(self):
        print("âš¡ [Engine] Initializing...")
        initial_ids = self.tokenizer(
            self.cfg.system_prompt_text, 
            return_tensors="pt", 
            add_special_tokens=False
        ).input_ids.to(self.logic.thinker_device)
        
        # Talker Init
        codec_bos = self.model.config.talker_config.codec_bos_id
        self.last_talker_token = torch.tensor([[codec_bos]], device=self.logic.talker_device)

        # Prefill Thinker (Text Only)
        with torch.no_grad():
            # Init ì‹œì—ëŠ” Feature ì—†ì´ Textë§Œ
            out = self.logic.thinker_step(
                input_ids=initial_ids,
                input_features=None,
                feature_attention_mask=None,
                past_key_values=None,
                step_idx=0
            )
            self.thinker_kv_cache = out.past_key_values
            self.thinker_step_count = initial_ids.shape[1]
            
        print("âœ… [Engine] Ready.")

    def _thinker_loop(self):
        print("ğŸ§  [Thinker Thread] Running...")
        while self.is_running:
            try:
                # â˜… Feature(Mel Spec)ë¥¼ ë°›ìŒ
                audio_features = self.input_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            with torch.no_grad():
                # [Step 1] Audio Feature ì…ë ¥ -> Thinker Forward
                # Feature Mask ìƒì„± (ì„±ê³µ ì½”ë“œ ì°¸ì¡°)
                time_len = audio_features.shape[2]
                feature_mask = torch.ones((1, time_len), device=self.logic.thinker_device, dtype=torch.long)

                # â˜… [ìˆ˜ì • í•µì‹¬] input_idsê°€ Noneì´ë©´ ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ, 
                # ì˜¤ë””ì˜¤ë§Œ ì²˜ë¦¬í•  ë•Œë„ í˜•ì‹ì„ ë§ì¶°ì¤˜ì•¼ í•¨.
                # Qwen3-OmniëŠ” ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œ input_idsë¥¼ ì•ˆ ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ,
                # transformers êµ¬í˜„ì²´ì— ë”°ë¼ input_idsë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆìŒ.
                # ì—¬ê¸°ì„œëŠ” input_ids=Noneìœ¼ë¡œ í˜¸ì¶œí•˜ë˜, logic.pyì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ìœ„ì„í–ˆìœ¼ë‚˜
                # ì—ëŸ¬ê°€ ë‚¬ìœ¼ë¯€ë¡œ ë”ë¯¸ input_idsë¥¼ ë„£ì–´ì¤Œ.
                
                # í•˜ì§€ë§Œ ë”ë¯¸ë¥¼ ë„£ìœ¼ë©´ í…ìŠ¤íŠ¸ê°€ ì„ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ,
                # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•: logic.pyì˜ thinker_step ìˆ˜ì •
                
                thinker_out = self.logic.thinker_step(
                    input_ids=None, 
                    input_features=audio_features,
                    feature_attention_mask=feature_mask,
                    past_key_values=self.thinker_kv_cache,
                    step_idx=self.thinker_step_count
                )
                self.thinker_kv_cache = thinker_out.past_key_values
                
                # Step Count ì¦ê°€ (ì„±ê³µ ì½”ë“œì—ì„œëŠ” ì˜¤ë””ì˜¤ ì²˜ë¦¬ í›„ +1ë§Œ í–ˆìŒ. ì •í™•íˆëŠ” +time_len ì´ì§€ë§Œ 
                # Qwen3 ìŠ¤íŠ¸ë¦¬ë° íŠ¹ì„±ìƒ ì••ì¶•ëœ í† í° ìˆ˜ë§Œí¼ ì¦ê°€ì‹œí‚¤ëŠ”ê²Œ ë§ìŒ. 
                # ì¼ë‹¨ 4í† í°(0.32s)ì— ëŒ€í•´ 1ìŠ¤í… ì¦ê°€ë¡œ ê°€ì •í•˜ê³  ì§„í–‰)
                # (ë§Œì•½ ìë„¤ ì„±ê³µ ì½”ë“œê°€ 1ìŠ¤í…ë§Œ ì¦ê°€ì‹œì¼°ë‹¤ë©´ 1ì´ ë§ìŒ)
                self.thinker_step_count += 4 # 4 audio tokens ì…ë ¥ë˜ì—ˆìœ¼ë¯€ë¡œ

                # [Step 2] Text Generation
                # ì²« í† í° ì˜ˆì¸¡ (ì˜¤ë””ì˜¤ í†µê³¼ ê²°ê³¼ì—ì„œ)
                next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                
                current_turn_hiddens = []
                
                # ì²« ë²ˆì§¸ ì˜ˆì¸¡ í† í° ì²˜ë¦¬
                if next_token.item() == self.cfg.silence_token_id:
                    pass # Silenceë©´ ë„˜ì–´ê°
                else:
                    # ì²« í† í°ì— ëŒ€í•œ Hidden State ì €ì¥
                    current_turn_hiddens.append(thinker_out.hidden_states[-1])
                    
                    # 2ë²ˆì§¸ í† í°ë¶€í„° ìƒì„± (ì„¤ì •ëœ ê°¯ìˆ˜ë§Œí¼)
                    for _ in range(self.cfg.text_output_tokens - 1):
                        thinker_out = self.logic.thinker_step(
                            input_ids=next_token,
                            input_features=None,
                            feature_attention_mask=None,
                            past_key_values=self.thinker_kv_cache,
                            step_idx=self.thinker_step_count
                        )
                        self.thinker_kv_cache = thinker_out.past_key_values
                        self.thinker_step_count += 1
                        
                        next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                        
                        if next_token.item() == self.cfg.silence_token_id:
                            break
                        
                        current_turn_hiddens.append(thinker_out.hidden_states[-1])

                # Talker Queueì— ë„£ê¸°
                if len(current_turn_hiddens) > 0:
                    stacked_hidden = torch.cat(current_turn_hiddens, dim=1)
                    self.hidden_queue.put(stacked_hidden)

    def _talker_loop(self):
        print("ğŸ‘„ [Talker Thread] Running...")
        while self.is_running:
            try:
                source_hidden = self.hidden_queue.get(timeout=0.1)
            except queue.Empty:
                continue
            
            with torch.no_grad():
                num_hiddens = source_hidden.shape[1]
                # Text 1ê°œë‹¹ Audio 2ê°œ (2:4 ë¹„ìœ¨)
                ratio = self.cfg.audio_output_tokens // self.cfg.text_output_tokens
                
                for i in range(num_hiddens):
                    one_hidden = source_hidden[:, i:i+1, :]
                    for _ in range(ratio):
                        codes, new_kv = self.logic.talker_step(
                            thinker_hidden=one_hidden,
                            past_key_values=self.talker_kv_cache,
                            step_idx=self.talker_step_count,
                            input_ids=self.last_talker_token
                        )
                        self.talker_kv_cache = new_kv
                        self.talker_step_count += 1
                        self.last_talker_token = codes[:, 0:1] # Layer 0 Code
                        
                        wav_bytes = self.logic.decode_audio(codes)
                        self.output_queue.put(wav_bytes)

    def start(self):
        if self.is_running: return
        self.is_running = True
        self.t_thinker = threading.Thread(target=self._thinker_loop, daemon=True)
        self.t_talker = threading.Thread(target=self._talker_loop, daemon=True)
        self.t_thinker.start()
        self.t_talker.start()
        print("ğŸš€ Engine Threads Started.")

    def stop(self):
        self.is_running = False
        if self.t_thinker: self.t_thinker.join()
        if self.t_talker: self.t_talker.join()
        print("ğŸ›‘ Engine Threads Stopped.")

    def push_audio(self, audio_features: torch.Tensor):
        self.input_queue.put(audio_features)

    def get_audio_output(self) -> Optional[bytes]:
        try:
            return self.output_queue.get_nowait()
        except queue.Empty:
            return None