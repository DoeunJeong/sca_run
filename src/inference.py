# scripts/run_test.py
import os
import sys
import time
import argparse
import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ëª¨ë“ˆ importë¥¼ ìœ„í•´)
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜: scripts/run_test.py -> ìƒìœ„(root) -> src ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "src")))

# [ìˆ˜ì • 1] íŒ¨í‚¤ì§€ ê²½ë¡œë¥¼ ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°(sca_core)ì— ë§ê²Œ ìˆ˜ì •
from src.inference import Qwen3OmniFullDuplexEngine, EngineConfig
from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor

def load_audio_file(file_path, target_sr=24000):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¦¬ìƒ˜í”Œë§í•¨"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Audio file not found: {file_path}")
        
    print(f"ğŸ“‚ Loading audio file: {file_path}")
    # librosaëŠ” float32 [-1, 1]ë¡œ ë¡œë“œí•¨
    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
    return audio, sr

def main():
    parser = argparse.ArgumentParser(description="Test Full-Duplex Engine with Audio File")
    parser.add_argument("--model-path", type=str, default="Qwen/Qwen2.5-Omni-7B", help="Path to model")
    parser.add_argument("--input-file", type=str, required=True, help="Input audio file (e.g. 3min_noisy.wav)")
    parser.add_argument("--output-file", type=str, default="output_response.wav", help="Output audio file")
    parser.add_argument("--device", type=str, default="cuda", help="Device to run")
    args = parser.parse_args()

    # 1. ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”¥ Loading Model from {args.model_path}...")
    
    # [ìˆ˜ì • 2] args.model_path ì‚¬ìš© ë° device_map ëª…ì‹œì  ì„¤ì •
    # (A40 2ì¥ì´ë©´ ì•„ë˜ì²˜ëŸ¼ ë¶„ì‚° ì„¤ì •, 1ì¥ì´ë©´ "auto")
    device_map = "auto" 
    
    model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
        args.model_path,
        device_map=device_map, 
        dtype='auto',          
        attn_implementation='flash_attention_2', 
        trust_remote_code=True
    )
    
    # 3. í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = Qwen3OmniMoeProcessor.from_pretrained(args.model_path, trust_remote_code=True)
    
    # 2. ì—”ì§„ ì´ˆê¸°í™”
    config = EngineConfig(audio_input_tokens=4, text_output_tokens=2, audio_output_tokens=4)
    
    # [ìˆ˜ì • 3] processor ìì²´ê°€ ì•„ë‹ˆë¼ processor.tokenizerë¥¼ ì „ë‹¬í•´ì•¼ í•¨
    engine = Qwen3OmniFullDuplexEngine(model, processor.tokenizer, config)
    
    # 3. ì˜¤ë””ì˜¤ ì¤€ë¹„ (Chunking)
    full_audio, sr = load_audio_file(args.input_file, target_sr=24000)
    
    # 4í† í° ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (0.32ì´ˆ)
    # 24000 * 0.32 = 7680 samples
    chunk_size = int(sr * 0.32) 
    
    chunks = [full_audio[i:i + chunk_size] for i in range(0, len(full_audio), chunk_size)]
    print(f"ğŸ“¦ Audio split into {len(chunks)} chunks (Chunk size: {chunk_size} samples)")

    # 4. í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì“°ë ˆë“œ ê°€ë™)
    engine.start()
    
    collected_output_audio = []
    start_time = time.time()
    
    try:
        # -- [Sender Loop] ì˜¤ë””ì˜¤ë¥¼ ì‹¤ì‹œê°„ì²˜ëŸ¼ ì¡°ê¸ˆì”© ë°€ì–´ë„£ìŒ --
        for i, chunk in enumerate(chunks):
            # ë§ˆì§€ë§‰ ì§œíˆ¬ë¦¬ íŒ¨ë”© (í•„ìš”ì‹œ)
            if len(chunk) < chunk_size:
                chunk = np.pad(chunk, (0, chunk_size - len(chunk)))
            
            # [ì¤‘ìš” ìˆ˜ì •] Raw Audio -> Processor -> Mel Spectrogram -> Audio Tower -> Embeddings
            # Engineì€ 'ì´ë¯¸ ì¸ì½”ë”©ëœ Feature(Embeddings)'ë¥¼ ë°›ë„ë¡ ì„¤ê³„í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ì „ì²˜ë¦¬ ìˆ˜í–‰
            
            with torch.no_grad():
                # 1. Processorë¥¼ í†µí•´ Mel Spectrogram ë³€í™˜ (Input Features)
                # sampling_rate í•„ìˆ˜ ì§€ì •
                processed_inputs = processor(
                    audios=[chunk], 
                    return_tensors="pt", 
                    sampling_rate=24000
                )
                
                # GPUë¡œ ì´ë™ ë° í˜•ë³€í™˜
                input_features = processed_inputs.input_features.to(args.device).to(model.dtype)
                feature_lens = processed_inputs.feature_attention_mask.sum(1).to(args.device)

                # 2. Audio Tower(Encoder) í†µê³¼ -> Embeddings ì¶”ì¶œ
                # Qwen3-Omni Audio TowerëŠ” (input_features, feature_lens)ë¥¼ ë°›ìŒ
                audio_embeds = model.audio_tower(
                    input_features, 
                    feature_lens=feature_lens
                ).last_hidden_state # [1, Seq, Dim]
            
            # ì—”ì§„ì— íˆ¬ì… (Non-blocking)
            engine.push_audio(audio_embeds)
            
            # ì‹¤ì‹œê°„ì„± ì‹œë®¬ë ˆì´ì…˜ (0.32ì´ˆ ëŒ€ê¸°)
            # ë„ˆë¬´ ë¹¨ë¦¬ ë„£ìœ¼ë©´ íê°€ ë„˜ì¹  ìˆ˜ ìˆê³ , ë„ˆë¬´ ëŠë¦¬ë©´ ëŠê¹€.
            # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì•½ê°„ ë¹ ë¥´ê²Œ(0.1ì´ˆ) ë„£ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥
            # time.sleep(0.1) 
            
            # -- [Receiver Loop] ìƒì„±ëœ ì˜¤ë””ì˜¤ ìˆ˜ê±° --
            # Sender ë£¨í”„ í•œ ë²ˆ ëŒ ë•Œë§ˆë‹¤ ì¶œë ¥ íë¥¼ ë¹„ìš¸ ë•Œê¹Œì§€ í™•ì¸
            while True:
                out_bytes = engine.get_audio_output()
                if out_bytes is None:
                    break
                
                # Bytes -> Numpy ë³€í™˜ (float32 [-1, 1]ë¡œ ë³€í™˜ ê°€ì •)
                # Code2Wav ì¶œë ¥ì´ int16 ë³€í™˜ëœ bytesë¼ë©´ ì•„ë˜ì²˜ëŸ¼ ë³µì›
                out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
                collected_output_audio.append(out_np)
                print(f"ğŸ”Š Received output chunk ({len(out_np)} samples) at step {i}")

        # ëª¨ë“  ì…ë ¥ ì „ì†¡ í›„ ì ì‹œ ëŒ€ê¸° (ì”ì—¬ ì¶œë ¥ ìˆ˜ê±°)
        print("â³ Waiting for remaining outputs...")
        time.sleep(3.0) # ì¶©ë¶„íˆ ê¸°ë‹¤ë ¤ì¤Œ
        
        # ë‚¨ì€ê±° ì‹¹ ê¸ì–´ëª¨ìœ¼ê¸°
        while True:
            out_bytes = engine.get_audio_output()
            if out_bytes is None: break
            out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
            collected_output_audio.append(out_np)

    except KeyboardInterrupt:
        print("ğŸ›‘ Test interrupted")
    except Exception as e:
        print(f"âŒ Error occurred: {e}")
    finally:
        engine.stop()
    
    # 5. ê²°ê³¼ ì €ì¥
    if collected_output_audio:
        final_audio = np.concatenate(collected_output_audio)
        print(f"ğŸ’¾ Saving {len(final_audio)} samples ({len(final_audio)/24000:.1f}s) to {args.output_file}")
        sf.write(args.output_file, final_audio, 24000)
    else:
        print("âš ï¸ No audio generated! (Check if silence token logic is working too strictly)")

    print(f"âœ… Test Finished. Total time: {time.time() - start_time:.2f}s")

if __name__ == "__main__":
    main()